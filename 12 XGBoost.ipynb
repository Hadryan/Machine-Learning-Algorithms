{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#XGBoost原理\" data-toc-modified-id=\"XGBoost原理-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>XGBoost原理</a></span><ul class=\"toc-item\"><li><span><a href=\"#提升方法（Boosting）\" data-toc-modified-id=\"提升方法（Boosting）-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>提升方法（Boosting）</a></span><ul class=\"toc-item\"><li><span><a href=\"#定义\" data-toc-modified-id=\"定义-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>定义</a></span></li><li><span><a href=\"#加法模型\" data-toc-modified-id=\"加法模型-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>加法模型</a></span></li><li><span><a href=\"#前向分步算法\" data-toc-modified-id=\"前向分步算法-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>前向分步算法</a></span></li></ul></li><li><span><a href=\"#提升决策树-（BDT，Boosting-Decision-Tree）\" data-toc-modified-id=\"提升决策树-（BDT，Boosting-Decision-Tree）-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>提升决策树 （BDT，Boosting Decision Tree）</a></span><ul class=\"toc-item\"><li><span><a href=\"#定义\" data-toc-modified-id=\"定义-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>定义</a></span></li><li><span><a href=\"#前向分步算法\" data-toc-modified-id=\"前向分步算法-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>前向分步算法</a></span></li><li><span><a href=\"#回归问题的提升决策树\" data-toc-modified-id=\"回归问题的提升决策树-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>回归问题的提升决策树</a></span></li><li><span><a href=\"#回归问题的提升决策树算法\" data-toc-modified-id=\"回归问题的提升决策树算法-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>回归问题的提升决策树算法</a></span></li></ul></li><li><span><a href=\"#梯度提升决策树-（GBDT，Gradient-Boosting-Decision-Tree）\" data-toc-modified-id=\"梯度提升决策树-（GBDT，Gradient-Boosting-Decision-Tree）-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>梯度提升决策树 （GBDT，Gradient Boosting Decision Tree）</a></span><ul class=\"toc-item\"><li><span><a href=\"#定义\" data-toc-modified-id=\"定义-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>定义</a></span></li><li><span><a href=\"#梯度提升算法\" data-toc-modified-id=\"梯度提升算法-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>梯度提升算法</a></span></li></ul></li><li><span><a href=\"#极限梯度提升（XGBoost，eXtreme-Gradient-Boosting）\" data-toc-modified-id=\"极限梯度提升（XGBoost，eXtreme-Gradient-Boosting）-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>极限梯度提升（XGBoost，eXtreme Gradient Boosting）</a></span><ul class=\"toc-item\"><li><span><a href=\"#定义\" data-toc-modified-id=\"定义-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>定义</a></span></li><li><span><a href=\"#正则化目标函数\" data-toc-modified-id=\"正则化目标函数-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>正则化目标函数</a></span></li><li><span><a href=\"#二阶泰勒展开\" data-toc-modified-id=\"二阶泰勒展开-1.4.3\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>二阶泰勒展开</a></span></li><li><span><a href=\"#分裂查找的精确贪婪算法\" data-toc-modified-id=\"分裂查找的精确贪婪算法-1.4.4\"><span class=\"toc-item-num\">1.4.4&nbsp;&nbsp;</span>分裂查找的精确贪婪算法</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "$$\n",
    "\\begin{align}\n",
    "X\\!G\\!Boost&=eXtreme+GBDT\\\\\n",
    "&=eXtreme+(Gradient+BDT) \\\\\n",
    "&=eXtreme+Gradient+(Boosting+DecisionTree)\n",
    "\\end{align}\n",
    "$$  \n",
    "\n",
    "$$DecisionTree+Boosting \\to BDT \\to GBDT \\to X\\!G\\!Boost$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提升方法（Boosting）\n",
    "\n",
    "#### 定义\n",
    "提升方法使用加法模型 + 前向分步算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加法模型\n",
    "\n",
    "<font color=red>**预测模型**</font>\n",
    "$$f\\left(x\\right)=\\sum_{m=1}^M\\beta_m b\\left(x;\\gamma_m\\right) \\tag{1.1}$$\n",
    "\n",
    "其中，$b\\left(x;\\gamma_m\\right)$为基函数，比如线性回归，逻辑回归，决策树，神经网络等；$\\gamma_m$为基函数的参数，$\\beta_m$为基函数的系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**模型求解**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在给定训练数据$\\{\\left(x_i,y_i\\right)\\}_{i=1}^N$及损失函数$L\\left(y, f\\left(x\\right)\\right)$的条件下，学习加法模型$f\\left(x\\right)$成为经验风险极小化问题：\n",
    "\n",
    "$$\\min_{\\beta_m,\\gamma_m}\\sum_{i=1}^N L\\left(y_i, \\sum_{m=1}^M\\beta_m b\\left(x_i;\\gamma_m\\right)\\right)\\tag{1.2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**模型问题**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为加法模型是由很多个模型累加而成的，比较难以优化，所以我们利用前向分步算法求解这一优化问题。其思路是：因为学习的是加法模型，可以从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式（1.2），则可以简化优化复杂度。具体地，每步只需优化如下损失函数：\n",
    "$$\\min_{\\beta,\\gamma}\\sum_{i=1}^N L\\left(y_i, \\beta b\\left(x_i;\\gamma\\right)\\right)\\tag{1.3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前向分步算法\n",
    "\n",
    "输入：训练数据集$T=\\{\\left(x_1,y_1\\right),\\left(x_2,y_2\\right),\\dots,\\left(x_N,y_N\\right)\\}$；损失函数$L\\left(y,f\\left(x\\right)\\right)$；基函数集合$\\{b\\left(x;\\gamma\\right)\\}$；   \n",
    "输出：加法模型$f\\left(x\\right)$  \n",
    "（1）初始化$f_0\\left(x\\right)=0$  \n",
    "（2）对$m=1,2,\\dots,M$  \n",
    "&emsp;&emsp;（a）极小化损失函数，以得到参数$\\beta_m$，$\\gamma_m$ \n",
    "$$\\left(\\beta_m,\\gamma_m\\right)=\\mathop{\\arg\\min}_{\\beta,\\gamma} \\sum_{i=1}^N L\\left(y_i,  f_{m-1}\\left(x_i\\right)+\\beta b\\left(x_i;\\gamma\\right)\\right) \\tag{1.4}$$  \n",
    "\n",
    "&emsp;&emsp;（b）更新\n",
    "$$f_m\\left(x\\right)=f_{m-1}\\left(x\\right)+\\beta_m b\\left(x;\\gamma_m\\right) \\tag{1.5}$$ \n",
    "\n",
    "（3）得到加法模型  \n",
    "$$f\\left(x\\right)=f_M\\left(x\\right)=\\sum_{m=1}^M\\beta_m b\\left(x;\\gamma_m\\right) \\tag{1.6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**总结：前向分步算法将<font color=red>同时求解</font>从$m=1$到$M$所有参数$\\beta_m,\\gamma_m$的优化问题简化为<font color=red>逐次求解</font>各个$\\beta_m, \\gamma_m$的优化问题。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提升决策树 （BDT，Boosting Decision Tree）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义\n",
    "以<font color=red>**决策树为基函数（基函数权重系数 = 1）**</font>的提升方法为提升决策树。提升决策树模型可以表示为决策树的加法模型：  \n",
    "\n",
    "$$f_M=\\sum_{m=1}^M T\\left(x;\\Theta_m\\right) \\tag{2.1}$$  \n",
    "其中，$T\\left(x;\\Theta_m\\right)$表示决策树；$\\Theta_m$为决策树的参数；$M$为树的个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已知训练数据集$T=\\{\\left(x_1,y_1\\right),\\left(x_2,y_2\\right),\\dots\\left(x_N,y_N\\right)\\}$，$x_i\\in\\mathcal{X}\\subseteq\\mathbb{R}^n$，$\\mathcal{X}$为输入空间，$y_i\\in\\mathcal{Y}\\subseteq\\mathbb{R}$，$\\mathcal{Y}$为输出空间。如果将输入空间$\\mathcal{X}$划分为$J$个互不相交的区域$R_1,R_2,\\dots,R_J$，并且在每个区域上确定输出的常量$c_j$，那么决策树可表示为\n",
    "$$T\\left(x;\\Theta\\right)=\\sum_{j=1}^J c_j I\\left(x\\in R_j\\right) \\tag{2.4}$$\n",
    "其中，参数$\\Theta=\\{\\left(R_1,c_1\\right),\\left(R_2,c_2\\right),\\dots,\\left(R_J,c_J\\right)\\}$表示决策树的区域划分和各区域上的常量值。$J$是决策树的复杂度即叶子结点个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=BDT.png width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前向分步算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提升决策树采用前向分步算法。首先确定初始提升决策树$f_0\\left(x\\right)=0$（只有一个节点的树），第$m$步的模型是\n",
    "\n",
    "$$f_m\\left(x\\right)=f_{m-1}\\left(x\\right)+T\\left(x;\\Theta_m\\right) \\tag{2.2}$$\n",
    "\n",
    "其中，$f_{m-1}\\left(x\\right)$为当前模型，通过经验风险极小化确定下一棵决策树的参数$\\Theta_m$，\n",
    "$$\\hat{\\Theta}_m=\\mathop{\\arg\\min}_{\\Theta_m}\\sum_{i=1}^N L\\left(y_i, f_{m-1}\\left(x_i\\right)+T\\left(x_i;\\Theta_m\\right)\\right) \\tag{2.3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提升决策树使用以下前向分步算法：\n",
    "$$\\begin{align}\n",
    "f_0\\left(x\\right)&=0 \\\\\n",
    "f_m\\left(x\\right)&=f_{m-1}\\left(x\\right)+T\\left(x;\\Theta_m\\right),\\quad m=1,2,\\dots,M        \\\\\n",
    "f_M\\left(x\\right)&=\\sum_{m=1}^M T\\left(x;\\Theta_m\\right)\n",
    "\\end{align}$$  \n",
    "在前向分步算法的第$m$步，给定当前模型$f_{m-1}\\left(x\\right)$，需要求解  \n",
    "$$\\hat{\\Theta}_m=\\mathop{\\arg\\min}_{\\Theta_m}\\sum_{i=1}^N L\\left(y_i,f_{m-1}\\left(x_i\\right)+T\\left(x_i;\\Theta_m\\right)\\right)$$\n",
    "得到$\\hat{\\Theta}_m$，即第$m$棵树的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 回归问题的提升决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当采用平方误差(MSE)损失函数时，\n",
    "$$L\\left(y,f\\left(x\\right)\\right)=\\left(y-f\\left(x\\right)\\right)^2$$\n",
    "其损失变为\n",
    "$$\\begin{align}\n",
    "L\\left(y,f_{m-1}\\left(x\\right)+T\\left(x;\\Theta_m\\right)\\right) \n",
    "&=\\left[y-f_{m-1}\\left(x\\right)-T\\left(x;\\Theta_m\\right)\\right]^2 \\\\\n",
    "&=\\left[r-T\\left(x;\\Theta_m\\right)\\right]^2\n",
    "\\end{align}$$\n",
    "其中，\n",
    "$$r=y-f_{m-1}\\left(x\\right) \\tag{2.5}$$\n",
    "\n",
    "$r$是当前模型拟合数据的残差（residual），也是上一个模型没有学好的部分。对回归问题的提升决策树，只需要简单地拟合当前模型的残差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 回归问题的提升决策树算法  \n",
    "输入：训练数据集$T=\\{\\left(x_1,y_1\\right),\\left(x_2,y_2\\right),\\dots,\\left(x_N,y_N\\right)\\}$；   \n",
    "输出：提升决策树$f_M\\left(x\\right)$  \n",
    "（1）初始化$f_0\\left(x\\right)=0$  \n",
    "（2）对$m=1,2,\\dots,M$ \n",
    "\n",
    "&emsp;&emsp;（a）按照式（2.5）计算残差\n",
    "$$r_{mi}=y_i-f_{m-1}\\left(x_i\\right), \\quad i=1,2,\\dots,N$$  \n",
    "\n",
    "&emsp;&emsp;（b）拟合残差$r_{mi}$学习一个回归树，得到$T\\left(x;\\Theta_m\\right)$  \n",
    "\n",
    "&emsp;&emsp;（c）更新$f_m\\left(x\\right)=f_{m-1}\\left(x\\right)+T\\left(x;\\Theta_m\\right) $  \n",
    "\n",
    "（3）得到回归提升决策树 \n",
    "$$f_M\\left(x\\right)=\\sum_{m=1}^M T\\left(x;\\Theta_m\\right)   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度提升决策树 （GBDT，Gradient Boosting Decision Tree）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义\n",
    "\n",
    "梯度提升算法使用损失函数的负梯度在当前模型的值\n",
    "$$-\\left[\\frac{\\partial L\\left(y,f\\left(x_i\\right)\\right)}{\\partial f\\left(x_i\\right)}\\right]_{f\\left(x\\right)=f_{m-1}\\left(x\\right)} \\tag{3.1}$$\n",
    "\n",
    "作为回归问题提升决策树算法中残差的近似值，拟合一个回归树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度提升算法  \n",
    "输入：训练数据集$T=\\{\\left(x_1,y_1\\right),\\left(x_2,y_2\\right),\\dots,\\left(x_N,y_N\\right)\\}$； 损失函数$L\\left(y,f\\left(x\\right)\\right)$  \n",
    "输出：梯度提升决策树$\\hat{f}\\left(x\\right)$  \n",
    "（1）初始化\n",
    "$$f_0\\left(x\\right)=\\mathop{\\arg\\min}_c\\sum_{i=1}^N L\\left(y_i,c\\right)$$\n",
    "（2）对$m=1,2,\\dots,M$  \n",
    "&emsp;&emsp;（a）对$i=1,2,\\dots,N$，计算\n",
    "$$r_{mi}=-\\left[\\frac{\\partial L\\left(y,f\\left(x_i\\right)\\right)}{\\partial f\\left(x_i\\right)}\\right]_{f\\left(x\\right)=f_{m-1}\\left(x\\right)}$$  \n",
    "&emsp;&emsp;（b)对$r_{mi}$拟合一个回归树，得到第$m$棵树的叶结点区域$R_{mj},j=1,2,\\dots,J$  \n",
    "&emsp;&emsp;（c）对$j=1,2,\\dots,J$，计算\n",
    "$$c_{mj}=\\mathop{\\arg\\min}_c\\sum_{x_i\\in R_{mj}} L\\left(y_i, f_{m-1}\\left(x_i\\right)+c\\right)$$  \n",
    "&emsp;&emsp;（d）更新$f_m\\left(x\\right)=f_{m-1}\\left(x\\right)+\\sum_{j=1}^J c_{mj} I\\left(x\\in R_{mj}\\right)$  \n",
    "\n",
    "（3）得到回归梯度提升决策树 \n",
    "$$\\hat{f}\\left(x\\right)=f_M\\left(x\\right)=\\sum_{m=1}^M \\sum_{j=1}^J c_{mj} I\\left(x\\in R_{mj}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 极限梯度提升（XGBoost，eXtreme Gradient Boosting）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义\n",
    "\n",
    "训练数据集$\\mathcal{D}=\\{\\left(\\mathbf{x}_i,y_i\\right)\\}$, 其中$\\mathbf{x}_i\\in\\mathbb{R}^m, y_i\\in\\mathbb{R}, {x}_i和{y}_i是列向量, \\left|\\mathcal{D}\\right|=n$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树模型\n",
    "$$f\\left(\\mathbf{x}\\right)=w_{q\\left(\\mathbf{x}\\right)} \\tag{4.1}$$\n",
    "\n",
    "其中，$q:\\mathbb{R}^m\\to \\{1,\\dots,T\\}$是有输入 $\\mathbf{x}$ 向叶子结点编号的映射，$w\\in\\mathbb{R}^T$是叶子结点向量，$T$为决策树叶子节点数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=XGBoost.png width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提升决策树模型预测输出\n",
    "$$\\hat{y}_i=\\phi\\left(\\mathbf{x}_i\\right)=\\sum_{k=1}^K f_k\\left(\\mathbf{x}_i\\right) \\tag{4.2}$$\n",
    "其中，$f_k\\left(\\mathbf{x}\\right)$为第$k$棵决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正则化目标函数\n",
    "$$\\mathcal{L}\\left(\\phi\\right)=\\sum_i l\\left(\\hat{y}_i,y_i\\right)+\\sum_k \\Omega\\left(f_k\\right) \\tag{4.3}$$\n",
    "\n",
    "其中，$\\Omega\\left(f\\right)=\\gamma T+\\frac{1}{2}\\lambda\\|w\\|^2=\\gamma T+\\frac{1}{2}\\lambda\\sum_{j=1}^T w_j^2$，限制叶子节点个数 $T$，和模型输出 $w$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第$t$轮**目标函数\n",
    "$$\\mathcal{L}^{\\left(t\\right)}=\\sum_{i=1}^n l\\left(y_i, \\hat{y}^{\\left(t-1\\right)}_i+f_t\\left(\\mathbf{x}_i\\right)\\right)+\\Omega\\left(f_t\\right) \\tag{4.4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二阶泰勒展开\n",
    "\n",
    "第$t$轮目标函数$\\mathcal{L}^{\\left(t\\right)}$在$\\hat{y}^{\\left(t-1\\right)}$处的二阶泰勒展开\n",
    "$$\\begin{align}\n",
    "\\mathcal{L}^{\\left(t\\right)}&\\simeq\\sum_{i=1}^n\\left[l\\left(y_i,\\hat{y}^{\\left(t-1\\right)}\\right)+\\partial_{\\hat{y}^{\\left(t-1\\right)}}l\\left(y_i,\\hat{y}^{\\left(t-1\\right)}\\right) f_t\\left(\\mathbf{x}_i\\right)+\\frac{1}{2}\\partial^2_{\\hat{y}^{\\left(t-1\\right)}}l\\left(y_i,\\hat{y}^{\\left(t-1\\right)}\\right) f^2_t\\left(\\mathbf{x}_i\\right)\\right]+\\Omega\\left(f_t\\right)  \\tag{4.5}  \\\\\n",
    "&=\\sum_{i=1}^n\\left[l\\left(y_i,\\hat{y}^{\\left(t-1\\right)}\\right)+g_i f_t\\left(\\mathbf{x}_i\\right)+\\frac{1}{2}h_i f^2_t\\left(\\mathbf{x}_i\\right)\\right]+\\Omega\\left(f_t\\right)\n",
    "\\end{align}$$\n",
    "\n",
    "其中，一阶导数：$g_i=\\partial_{\\hat{y}^{\\left(t-1\\right)}}l\\left(y_i,\\hat{y}^{\\left(t-1\\right)}\\right),  二阶导数： h_i=\\partial^2_{\\hat{y}^{\\left(t-1\\right)}}l\\left(y_i,\\hat{y}^{\\left(t-1\\right)}\\right)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第$t$轮目标函数$\\mathcal{L}^{\\left(t\\right)}$的二阶泰勒展开移除关于\n",
    "$f_t\\left(\\mathbf{x}_i\\right)$常数项\n",
    "$$\\begin{align}\n",
    "\\tilde{\\mathcal{L}}^{\\left(t\\right)}&=\\sum_{i=1}^n\\left[g_i f_t\\left(\\mathbf{x}_i\\right)+\\frac{1}{2}h_i f^2_t\\left(\\mathbf{x}_i\\right)\\right]+\\Omega\\left(f_t\\right)  \\tag{4.6}\\\\\n",
    "&=\\sum_{i=1}^n\\left[g_i f_t\\left(\\mathbf{x}_i\\right)+\\frac{1}{2}h_i f^2_t\\left(\\mathbf{x}_i\\right)\\right]+\\gamma T+\\frac{1}{2}\\lambda\\sum_{j=1}^T w_j^2\n",
    "\\end{align} \\\\$$  \n",
    "定义叶结点$j$上的样本的下标集合$I_j=\\{i|q\\left(\\mathbf{x}_i\\right)=j\\}$，则目标函数可表示为按叶结点累加的形式\n",
    "$$\\tilde{\\mathcal{L}}^{\\left(t\\right)}=\\sum_{j=1}^T\\left[\\left(\\sum_{i\\in I_j}g_i\\right)w_j+\\frac{1}{2}\\left(\\sum_{i\\in I_j}h_i+\\lambda\\right)w_j^2\\right]+\\gamma T \\tag{4.7}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=XGBoost1.png width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于$$w_j^*=\\mathop{\\arg\\min}_{w_j}\\tilde{\\mathcal{L}}^{\\left(t\\right)}$$\n",
    "可令$$\\frac{\\partial\\tilde{\\mathcal{L}}^{\\left(t\\right)}}{\\partial w_j}=0$$\n",
    "得到每个叶结点$j$的最优分数为\n",
    "$$w_j^*=-\\frac{\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j} h_i+\\lambda} \\tag{4.8}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代入每个叶结点$j$的最优分数，得到最优化目标函数值\n",
    "$$\\tilde{\\mathcal{L}}^{\\left(t\\right)}\\left(q\\right)=-\\frac{1}{2}\\sum_{j=1}^T \n",
    "\\frac{\\left(\\sum_{i\\in I_j} g_i\\right)^2}{\\sum_{i\\in I_j} h_i+\\lambda}+\\gamma T \\tag{4.9}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设$I_L$和$I_R$分别为分裂后左右结点的实例集，令$I=I_L\\cup I_R$，则分裂后损失减少量由下式得出\n",
    "\n",
    "$$\\mathcal{L}_{split}=\\frac{1}{2}\\left[\\frac{\\left(\\sum_{i\\in I_L} g_i\\right)^2}{\\sum_{i\\in I_L}h_i+\\lambda}+\\frac{\\left(\\sum_{i\\in I_R} g_i\\right)^2}{\\sum_{i\\in I_R}h_i+\\lambda}-\\frac{\\left(\\sum_{i\\in I} g_i\\right)^2}{\\sum_{i\\in I}h_i+\\lambda}\\right]-\\gamma \\tag{4.10}$$\n",
    "用以评估待分裂结点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分裂查找的精确贪婪算法  \n",
    "输入：当前结点实例集$I$;特征维度$d$  \n",
    "输出：根据最大分值分裂  \n",
    "（1）$gain\\leftarrow 0$  \n",
    "（2）$G\\leftarrow\\sum_{i\\in I}g_i$，$H\\leftarrow\\sum_{i\\in I}h_i$  \n",
    "（3）for $k=1$ to $d$ do  \n",
    "（3.1）$G_L \\leftarrow 0$，$H_L \\leftarrow 0$  \n",
    "（3.2）for $j$ in sorted($I$, by $\\mathbf{x}_{jk}$) do  \n",
    "（3.2.1）$G_L \\leftarrow G_L+g_j$，$H_L \\leftarrow H_L+h_j$  \n",
    "（3.2.2）$G_R \\leftarrow G-G_L$，$H_R=H-H_L$  \n",
    "（3.2.3）$score \\leftarrow \\max\\left(score,\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{G^2}{H+\\lambda}\\right)$  \n",
    "（3.3）end  \n",
    "（4）end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "536.8px",
    "left": "148px",
    "top": "435.8px",
    "width": "268.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
